########### LOGISTIC REGRESSION #############

Testing on training set:
              precision    recall  f1-score   support

         0.0      0.320     0.693     0.438       427
         1.0      0.952     0.805     0.872      3216

    accuracy                          0.792      3643
   macro avg      0.636     0.749     0.655      3643
weighted avg      0.878     0.792     0.821      3643

auc macro 0.835
confusion matrix
[[ 296  131]
 [ 628 2588]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.334     0.713     0.455       143
         1.0      0.955     0.811     0.877      1072

    accuracy                          0.799      1215
   macro avg      0.645     0.762     0.666      1215
weighted avg      0.882     0.799     0.827      1215

auc macro 0.846
confusion matrix
[[102  41]
 [203 869]]
Model rank: 1
Mean validation score: 0.668 (std: 0.011)
Parameters: {'model__C': 9, 'model__dual': True, 'model__max_iter': 408, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'model__warm_start': True}

########### LOGISTIC REGRESSION END #############

########### SVC #############

Testing on training set:
              precision    recall  f1-score   support

         0.0      0.247     0.445     0.318       427
         1.0      0.918     0.820     0.866      3216

    accuracy                          0.776      3643
   macro avg      0.582     0.632     0.592      3643
weighted avg      0.839     0.776     0.802      3643

auc macro 0.706
confusion matrix
[[ 190  237]
 [ 579 2637]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.215     0.392     0.277       143
         1.0      0.909     0.809     0.856      1072

    accuracy                          0.760      1215
   macro avg      0.562     0.600     0.567      1215
weighted avg      0.827     0.760     0.788      1215

auc macro 0.655
confusion matrix
[[ 56  87]
 [205 867]]
Model rank: 1
Mean validation score: 0.646 (std: 0.015)
Parameters: {'model__C': 239, 'model__coef0': 0.002856502817182416, 'model__degree': 27, 'model__gamma': 'scale', 'model__kernel': 'rbf', 'model__max_iter': 1600}

########### SVC END #############

########### KNeighborsClassifier #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.656     1.000     0.792       427
         1.0      1.000     0.930     0.964      3216

    accuracy                          0.939      3643
   macro avg      0.828     0.965     0.878      3643
weighted avg      0.960     0.939     0.944      3643

auc macro 0.973
confusion matrix
[[ 427    0]
 [ 224 2992]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.286     0.427     0.343       143
         1.0      0.918     0.858     0.887      1072

    accuracy                          0.807      1215
   macro avg      0.602     0.642     0.615      1215
weighted avg      0.844     0.807     0.823      1215

auc macro 0.645
confusion matrix
[[ 61  82]
 [152 920]]
Model rank: 1
Mean validation score: 0.604 (std: 0.012)
Parameters: {'model__algorithm': 'ball_tree', 'model__leaf_size': 54, 'model__n_neighbors': 2, 'model__weights': 'uniform'}

########### KNeighborsClassifier END #############

########### RANDOM FOREST #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.889     0.974     0.930       427
         1.0      0.997     0.984     0.990      3216

    accuracy                          0.983      3643
   macro avg      0.943     0.979     0.960      3643
weighted avg      0.984     0.983     0.983      3643

auc macro 0.997
confusion matrix
[[ 416   11]
 [  52 3164]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.525     0.364     0.430       143
         1.0      0.918     0.956     0.937      1072

    accuracy                          0.886      1215
   macro avg      0.722     0.660     0.683      1215
weighted avg      0.872     0.886     0.877      1215

auc macro 0.832
confusion matrix
[[  52   91]
 [  47 1025]]
Model rank: 1
Mean validation score: 0.666 (std: 0.033)
Parameters: {'model__class_weight': 'balanced', 'model__criterion': 'entropy', 'model__max_features': 'log2', 'model__min_samples_leaf': 4, 'model__min_samples_split': 2, 'model__n_estimators': 107}


########### RANDOM FOREST END #############

########### ADABOOST  #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.629     0.258     0.365       427
         1.0      0.909     0.980     0.943      3216

    accuracy                          0.895      3643
   macro avg      0.769     0.619     0.654      3643
weighted avg      0.876     0.895     0.875      3643

auc macro 0.859
confusion matrix
[[ 110  317]
 [  65 3151]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.613     0.266     0.371       143
         1.0      0.909     0.978     0.942      1072

    accuracy                          0.894      1215
   macro avg      0.761     0.622     0.656      1215
weighted avg      0.874     0.894     0.875      1215

auc macro 0.845
confusion matrix
[[  38  105]
 [  24 1048]]
Model rank: 1
Mean validation score: 0.649 (std: 0.027)
Parameters: {'model__learning_rate': 0.8647762758764215, 'model__n_estimators': 45}


########### ADABOOST END  #############

########### MLP  #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.580     0.288     0.385       427
         1.0      0.911     0.972     0.941      3216

    accuracy                          0.892      3643
   macro avg      0.746     0.630     0.663      3643
weighted avg      0.873     0.892     0.876      3643

auc macro 0.833
confusion matrix
[[ 123  304]
 [  89 3127]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.671     0.343     0.454       143
         1.0      0.918     0.978     0.947      1072

    accuracy                          0.903      1215
   macro avg      0.794     0.660     0.700      1215
weighted avg      0.889     0.903     0.889      1215

auc macro 0.848
confusion matrix
[[  49   94]
 [  24 1048]]
Model rank: 1
Mean validation score: 0.665 (std: 0.005)
Parameters: {'model__alpha': 0.15765175415976684, 'model__early_stopping': True, 'model__hidden_layer_sizes': [123, 146], 'model__learning_rate': 'constant', 'model__learning_rate_init': 0.0051325500039257, 'model__max_iter': 372, 'model__solver': 'sgd'}


########### MLP END  #############

########### GRADIENT BOOSTING #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.716     0.419     0.529       427
         1.0      0.927     0.978     0.952      3216

    accuracy                          0.912      3643
   macro avg      0.821     0.699     0.740      3643
weighted avg      0.902     0.912     0.902      3643

auc macro 0.889
confusion matrix
[[ 179  248]
 [  71 3145]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.464     0.273     0.344       143
         1.0      0.908     0.958     0.932      1072

    accuracy                          0.877      1215
   macro avg      0.686     0.615     0.638      1215
weighted avg      0.856     0.877     0.863      1215

auc macro 0.811
confusion matrix
[[  39  104]
 [  45 1027]]
Model rank: 1
Mean validation score: 0.653 (std: 0.019)
Parameters: {'model__learning_rate': 0.1638482464618695, 'model__max_depth': 3, 'model__max_features': None, 'model__n_estimators': 96, 'model__subsample': 0.25}

########### GRADIENT BOOSTING END #############

########### XGB #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.571     0.492     0.528       427
         1.0      0.934     0.951     0.942      3216

    accuracy                          0.897      3643
   macro avg      0.752     0.721     0.735      3643
weighted avg      0.891     0.897     0.894      3643

auc macro 0.876
confusion matrix
[[ 210  217]
 [ 158 3058]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.540     0.469     0.502       143
         1.0      0.930     0.947     0.939      1072

    accuracy                          0.891      1215
   macro avg      0.735     0.708     0.720      1215
weighted avg      0.884     0.891     0.887      1215

auc macro 0.847
confusion matrix
[[  67   76]
 [  57 1015]]
Model rank: 1
Mean validation score: 0.686 (std: 0.006)
Parameters: {'model__alpha': 0.07252521171400944, 'model__booster': 'gbtree', 'model__eta': 0.26281541619692944, 'model__gamma': 0.177075126333618, 'model__lambda': 0.762381857279508, 'model__max_depth': 3, 'model__n_estimators': 46, 'model__scale_pos_weight': 0.4, 'model__subsample': 0.5}

########### XGB END #############
