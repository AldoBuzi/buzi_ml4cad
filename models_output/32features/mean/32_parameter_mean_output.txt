########### LOGISTIC REGRESSION #############

Testing on training set:
              precision    recall  f1-score   support

         0.0      0.481     0.517     0.498       505
         1.0      0.929     0.919     0.924      3494

    accuracy                          0.868      3999
   macro avg      0.705     0.718     0.711      3999
weighted avg      0.873     0.868     0.870      3999

auc macro 0.841
confusion matrix
[[ 261  244]
 [ 282 3212]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.489     0.503     0.496       169
         1.0      0.928     0.924     0.926      1165

    accuracy                          0.870      1334
   macro avg      0.708     0.713     0.711      1334
weighted avg      0.872     0.870     0.871      1334

auc macro 0.852
confusion matrix
[[  85   84]
 [  89 1076]]
Model rank: 1
Mean validation score: 0.684 (std: 0.005)
Parameters: {'model__C': 5, 'model__dual': True, 'model__max_iter': 182, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'model__warm_start': True}

########### LOGISTIC REGRESSION END #############

########### SVC #############

Testing on training set:
              precision    recall  f1-score   support

         0.0      0.251     0.442     0.320       505
         1.0      0.909     0.810     0.857      3494

    accuracy                          0.763      3999
   macro avg      0.580     0.626     0.589      3999
weighted avg      0.826     0.763     0.789      3999

auc macro 0.696
confusion matrix
[[ 223  282]
 [ 664 2830]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.271     0.467     0.343       169
         1.0      0.914     0.817     0.863      1165

    accuracy                          0.773      1334
   macro avg      0.592     0.642     0.603      1334
weighted avg      0.832     0.773     0.797      1334

auc macro 0.671
confusion matrix
[[ 79  90]
 [213 952]]
Model rank: 1
Mean validation score: 0.669 (std: 0.025)
Parameters: {'model__C': 152, 'model__coef0': 0.7569001354287388, 'model__degree': 89, 'model__gamma': 'scale', 'model__kernel': 'rbf', 'model__max_iter': 1600}

########### SVC END #############

########### KNeighborsClassifier #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.682     0.493     0.572       505
         1.0      0.930     0.967     0.948      3494

    accuracy                          0.907      3999
   macro avg      0.806     0.730     0.760      3999
weighted avg      0.898     0.907     0.900      3999

auc macro 0.937
confusion matrix
[[ 249  256]
 [ 116 3378]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.375     0.249     0.299       169
         1.0      0.896     0.940     0.917      1165

    accuracy                          0.852      1334
   macro avg      0.636     0.594     0.608      1334
weighted avg      0.830     0.852     0.839      1334

auc macro 0.688
confusion matrix
[[  42  127]
 [  70 1095]]
Model rank: 1
Mean validation score: 0.630 (std: 0.008)
Parameters: {'model__algorithm': 'ball_tree', 'model__leaf_size': 11, 'model__n_neighbors': 4, 'model__weights': 'uniform'}

########### KNeighborsClassifier END #############

########### RANDOM FOREST #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.859     0.964     0.909       505
         1.0      0.995     0.977     0.986      3494

    accuracy                          0.975      3999
   macro avg      0.927     0.971     0.947      3999
weighted avg      0.978     0.975     0.976      3999

auc macro 0.995
confusion matrix
[[ 487   18]
 [  80 3414]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.573     0.420     0.485       169
         1.0      0.919     0.955     0.936      1165

    accuracy                          0.887      1334
   macro avg      0.746     0.687     0.711      1334
weighted avg      0.875     0.887     0.879      1334

auc macro 0.865
confusion matrix
[[  71   98]
 [  53 1112]]
Model rank: 1
Mean validation score: 0.691 (std: 0.009)
Parameters: {'model__class_weight': 'balanced', 'model__criterion': 'gini', 'model__max_features': 'log2', 'model__min_samples_leaf': 4, 'model__min_samples_split': 3, 'model__n_estimators': 156}


########### RANDOM FOREST END #############

########### ADABOOST  #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.638     0.311     0.418       505
         1.0      0.907     0.975     0.940      3494

    accuracy                          0.891      3999
   macro avg      0.773     0.643     0.679      3999
weighted avg      0.873     0.891     0.874      3999

auc macro 0.852
confusion matrix
[[ 157  348]
 [  89 3405]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.629     0.260     0.368       169
         1.0      0.901     0.978     0.938      1165

    accuracy                          0.887      1334
   macro avg      0.765     0.619     0.653      1334
weighted avg      0.867     0.887     0.866      1334

auc macro 0.848
confusion matrix
[[  44  125]
 [  26 1139]]
Model rank: 1
Mean validation score: 0.677 (std: 0.000)
Parameters: {'model__learning_rate': 1.1618741071985597, 'model__n_estimators': 20}

########### ADABOOST END  #############

########### MLP  #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.633     0.269     0.378       505
         1.0      0.902     0.977     0.938      3494

    accuracy                          0.888      3999
   macro avg      0.768     0.623     0.658      3999
weighted avg      0.868     0.888     0.868      3999

auc macro 0.830
confusion matrix
[[ 136  369]
 [  79 3415]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.727     0.284     0.409       169
         1.0      0.905     0.985     0.943      1165

    accuracy                          0.896      1334
   macro avg      0.816     0.634     0.676      1334
weighted avg      0.882     0.896     0.875      1334

auc macro 0.854
confusion matrix
[[  48  121]
 [  18 1147]]
Model rank: 1
Mean validation score: 0.692 (std: 0.003)
Parameters: {'model__alpha': 0.9728686659277058, 'model__early_stopping': True, 'model__hidden_layer_sizes': [288, 88], 'model__learning_rate': 'constant', 'model__learning_rate_init': 0.0014712857484158568, 'model__max_iter': 489, 'model__solver': 'adam'}


########### MLP END  #############

########### GRADIENT BOOSTING #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.739     0.376     0.499       505
         1.0      0.916     0.981     0.947      3494

    accuracy                          0.904      3999
   macro avg      0.828     0.679     0.723      3999
weighted avg      0.894     0.904     0.891      3999

auc macro 0.878
confusion matrix
[[ 190  315]
 [  67 3427]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.623     0.284     0.390       169
         1.0      0.904     0.975     0.938      1165

    accuracy                          0.888      1334
   macro avg      0.764     0.630     0.664      1334
weighted avg      0.868     0.888     0.869      1334

auc macro 0.862
confusion matrix
[[  48  121]
 [  29 1136]]
Model rank: 1
Mean validation score: 0.664 (std: 0.006)
Parameters: {'model__learning_rate': 0.22648597411240698, 'model__max_depth': 2, 'model__max_features': 'log2', 'model__n_estimators': 93, 'model__subsample': 0.5}

########### GRADIENT BOOSTING END #############

########### XGB #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.542     0.550     0.546       505
         1.0      0.935     0.933     0.934      3494

    accuracy                          0.884      3999
   macro avg      0.738     0.742     0.740      3999
weighted avg      0.885     0.884     0.885      3999

auc macro 0.877
confusion matrix
[[ 278  227]
 [ 235 3259]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.512     0.485     0.498       169
         1.0      0.926     0.933     0.929      1165

    accuracy                          0.876      1334
   macro avg      0.719     0.709     0.714      1334
weighted avg      0.874     0.876     0.875      1334

auc macro 0.862
confusion matrix
[[  82   87]
 [  78 1087]]
Model rank: 1
Mean validation score: 0.706 (std: 0.009)
Parameters: {'model__alpha': 0.4246715792610688, 'model__booster': 'gbtree', 'model__eta': 0.38116393902047846, 'model__gamma': 0.0797483704737714, 'model__lambda': 0.7070515345251172, 'model__max_depth': 2, 'model__n_estimators': 85, 'model__scale_pos_weight': 0.4, 'model__subsample': 0.25}

########### XGB END #############
