########### LOGISTIC REGRESSION #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.332     0.681     0.447       427
         1.0      0.951     0.818     0.879      3216

    accuracy                          0.802      3643
   macro avg      0.642     0.750     0.663      3643
weighted avg      0.878     0.802     0.829      3643

auc macro 0.831
confusion matrix
[[ 291  136]
 [ 585 2631]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.344     0.671     0.455       143
         1.0      0.950     0.829     0.885      1072

    accuracy                          0.811      1215
   macro avg      0.647     0.750     0.670      1215
weighted avg      0.878     0.811     0.835      1215

auc macro 0.829
confusion matrix
[[ 96  47]
 [183 889]]
Model rank: 1
Mean validation score: 0.666 (std: 0.001)
Parameters: {'model__C': 6, 'model__dual': True, 'model__max_iter': 63, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'model__warm_start': False}

########### LOGISTIC REGRESSION END #############

########### SVC #############

Testing on training set:
              precision    recall  f1-score   support

         0.0      0.263     0.562     0.358       427
         1.0      0.931     0.790     0.855      3216

    accuracy                          0.764      3643
   macro avg      0.597     0.676     0.607      3643
weighted avg      0.853     0.764     0.797      3643

auc macro 0.733
confusion matrix
[[ 240  187]
 [ 674 2542]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.206     0.455     0.283       143
         1.0      0.913     0.766     0.833      1072

    accuracy                          0.729      1215
   macro avg      0.559     0.610     0.558      1215
weighted avg      0.830     0.729     0.768      1215

auc macro 0.671
confusion matrix
[[ 65  78]
 [251 821]]
Model rank: 1
Mean validation score: 0.663 (std: 0.005)
Parameters: {'model__C': 418, 'model__coef0': 0.620625258930853, 'model__degree': 100, 'model__gamma': 'auto', 'model__kernel': 'rbf', 'model__max_iter': 800}

########### SVC END #############

########### KNeighborsClassifier #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.654     1.000     0.791       427
         1.0      1.000     0.930     0.964      3216

    accuracy                          0.938      3643
   macro avg      0.827     0.965     0.877      3643
weighted avg      0.959     0.938     0.943      3643

auc macro 0.974
confusion matrix
[[ 427    0]
 [ 226 2990]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.273     0.399     0.324       143
         1.0      0.915     0.858     0.885      1072

    accuracy                          0.804      1215
   macro avg      0.594     0.628     0.605      1215
weighted avg      0.839     0.804     0.819      1215

auc macro 0.633
confusion matrix
[[ 57  86]
 [152 920]]
Model rank: 1
Mean validation score: 0.635 (std: 0.001)
Parameters: {'model__algorithm': 'ball_tree', 'model__leaf_size': 54, 'model__n_neighbors': 2, 'model__weights': 'uniform'}

########### KNeighborsClassifier END #############

########### RANDOM FOREST #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.864     0.956     0.908       427
         1.0      0.994     0.980     0.987      3216

    accuracy                          0.977      3643
   macro avg      0.929     0.968     0.947      3643
weighted avg      0.979     0.977     0.978      3643

auc macro 0.995
confusion matrix
[[ 408   19]
 [  64 3152]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.486     0.357     0.411       143
         1.0      0.917     0.950     0.933      1072

    accuracy                          0.880      1215
   macro avg      0.701     0.653     0.672      1215
weighted avg      0.866     0.880     0.872      1215

auc macro 0.812
confusion matrix
[[  51   92]
 [  54 1018]]
Model rank: 1
Mean validation score: 0.700 (std: 0.016)
Parameters: {'model__class_weight': 'balanced', 'model__criterion': 'entropy', 'model__max_features': 'log2', 'model__min_samples_leaf': 4, 'model__min_samples_split': 2, 'model__n_estimators': 61}


########### RANDOM FOREST END #############

########### ADABOOST  #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.645     0.302     0.411       427
         1.0      0.913     0.978     0.945      3216

    accuracy                          0.899      3643
   macro avg      0.779     0.640     0.678      3643
weighted avg      0.882     0.899     0.882      3643

auc macro 0.874
confusion matrix
[[ 129  298]
 [  71 3145]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.537     0.252     0.343       143
         1.0      0.907     0.971     0.938      1072

    accuracy                          0.886      1215
   macro avg      0.722     0.611     0.640      1215
weighted avg      0.863     0.886     0.868      1215

auc macro 0.796
confusion matrix
[[  36  107]
 [  31 1041]]
Model rank: 1
Mean validation score: 0.668 (std: 0.017)
Parameters: {'model__learning_rate': 1.1916503309286282, 'model__n_estimators': 65}


########### ADABOOST END  #############

########### MLP  #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.731     0.286     0.411       427
         1.0      0.912     0.986     0.948      3216

    accuracy                          0.904      3643
   macro avg      0.821     0.636     0.679      3643
weighted avg      0.891     0.904     0.885      3643

auc macro 0.864
confusion matrix
[[ 122  305]
 [  45 3171]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.673     0.231     0.344       143
         1.0      0.906     0.985     0.944      1072

    accuracy                          0.896      1215
   macro avg      0.790     0.608     0.644      1215
weighted avg      0.878     0.896     0.873      1215

auc macro 0.814
confusion matrix
[[  33  110]
 [  16 1056]]
Model rank: 1
Mean validation score: 0.672 (std: 0.000)
Parameters: {'model__alpha': 0.165752231993488, 'model__early_stopping': True, 'model__hidden_layer_sizes': [262, 63], 'model__learning_rate': 'adaptive', 'model__learning_rate_init': 0.0026264621646941563, 'model__max_iter': 342, 'model__solver': 'adam'}

########### MLP END  #############

########### GRADIENT BOOSTING #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.763     0.354     0.483       427
         1.0      0.920     0.985     0.952      3216

    accuracy                          0.911      3643
   macro avg      0.841     0.670     0.717      3643
weighted avg      0.901     0.911     0.897      3643

auc macro 0.876
confusion matrix
[[ 151  276]
 [  47 3169]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.590     0.252     0.353       143
         1.0      0.907     0.977     0.941      1072

    accuracy                          0.891      1215
   macro avg      0.749     0.614     0.647      1215
weighted avg      0.870     0.891     0.872      1215

auc macro 0.814
confusion matrix
[[  36  107]
 [  25 1047]]
Model rank: 1
Mean validation score: 0.672 (std: 0.001)
Parameters: {'model__learning_rate': 0.09185154934574266, 'model__max_depth': 3, 'model__max_features': None, 'model__n_estimators': 84, 'model__subsample': 0.25}

########### GRADIENT BOOSTING END #############

########### XGB #############

Fitting 2 folds for each of 5000 candidates, totalling 10000 fits
Testing on training set:
              precision    recall  f1-score   support

         0.0      0.662     0.890     0.759       427
         1.0      0.985     0.940     0.962      3216

    accuracy                          0.934      3643
   macro avg      0.823     0.915     0.860      3643
weighted avg      0.947     0.934     0.938      3643

auc macro 0.976
confusion matrix
[[ 380   47]
 [ 194 3022]]
Testing on validation set:
              precision    recall  f1-score   support

         0.0      0.386     0.510     0.440       143
         1.0      0.932     0.892     0.911      1072

    accuracy                          0.847      1215
   macro avg      0.659     0.701     0.676      1215
weighted avg      0.868     0.847     0.856      1215

auc macro 0.822
confusion matrix
[[ 73  70]
 [116 956]]
Model rank: 1
Mean validation score: 0.704 (std: 0.000)
Parameters: {'model__alpha': 0.24680855365947124, 'model__booster': 'dart', 'model__eta': 0.2477457170522508, 'model__gamma': 0.054843779888438804, 'model__lambda': 1.9207551518163506, 'model__max_depth': 6, 'model__n_estimators': 64, 'model__scale_pos_weight': 0.2, 'model__subsample': 0.75}

########### XGB END #############
